{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from encoder import TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device to gpu if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() \n",
    "                      else 'mps' if torch.backends.mps.is_available() \n",
    "                      else 'cpu')\n",
    "print(\"You are using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layer Tutorial Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.12, -0.76,  1.34,  0.58, -1.21,  0.44, -0.09,  0.72, -0.33],\n",
      "        [-0.58,  1.11,  0.30, -0.69,  0.93, -0.37, -1.05,  0.57,  0.22],\n",
      "        [ 0.48, -1.14,  0.26,  0.71, -0.53,  1.38, -0.66,  0.97, -0.28]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The matrix stored by the embedding layer.\n",
    "embedding_weights = torch.tensor([\n",
    "    [ 0.12, -0.76,  1.34,  0.58, -1.21,  0.44, -0.09,  0.72, -0.33],\n",
    "    [-0.45,  1.05,  0.39, -0.97,  0.18, -1.56,  0.87,  0.23, -0.12],\n",
    "    [ 1.23, -0.64,  0.07,  0.92, -0.31,  0.51, -1.22,  0.84, -0.77],\n",
    "    [-0.19,  0.34,  0.88, -1.03,  1.15, -0.42,  0.65, -0.91,  0.00],\n",
    "    [ 0.48, -1.14,  0.26,  0.71, -0.53,  1.38, -0.66,  0.97, -0.28],\n",
    "    [ 0.16,  0.62, -0.85,  1.04, -0.11, -0.76,  0.89, -1.34,  0.43],\n",
    "    [-0.58,  1.11,  0.30, -0.69,  0.93, -0.37, -1.05,  0.57,  0.22],\n",
    "    [ 0.35, -0.99,  0.79,  0.18, -0.82,  1.20, -0.48,  0.61, -0.13]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Initializing the embedding layer with the matrix above.\n",
    "embedding_layer = nn.Embedding(num_embeddings = 8, embedding_dim = 9)\n",
    "embedding_layer.weight.data = embedding_weights\n",
    "\n",
    "# Example of sequence input to embedding layer.\n",
    "input = torch.tensor([0,6,4])\n",
    "output = embedding_layer(input)\n",
    "\n",
    "torch.set_printoptions(precision=2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Encoder Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input: torch.Size([32, 43])\n",
      "Shape of output: torch.Size([32, 43, 512])\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we use a simulated batch of sentences, just to demonstrate usage. \n",
    "\n",
    "N = 32 #batch size \n",
    "T = 43 #sentence length\n",
    "vocab_size = 1000\n",
    "sentences = torch.randint(0, vocab_size, (N, T)).to(device)\n",
    "print(\"Shape of input:\", sentences.shape)\n",
    "\n",
    "# Model parameters. In this example we use the default values, which are set to those used in the base model of the paper.\n",
    "context_size = 43\n",
    "\n",
    "# Sample usage. Note the shapes of the input and output.\n",
    "model = TransformerEncoder(vocab_size, context_size).to(device)\n",
    "\n",
    "output = model(sentences)\n",
    "print(\"Shape of output:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embeddings): Embedding(1000, 512)\n",
       "  (positional_encodings): Embedding(43, 512)\n",
       "  (encoder_stack): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (multihead_attention_sublayer): AttentionSubLayer(\n",
       "        (multihead_attention): MultiHeadAttention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedforward_sublayer): FeedForwardSubLayer(\n",
       "        (feedforward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
